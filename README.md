# <h1 align="center"> <img src="./static/images/favicon.png" width="32" height="32" style="vertical-align: middle;"> VM-SHGS: Underwater 3D Scene Reconstruction with VM-SHGS</h1>

<h5 align="center">

Integrated neural rendering framework for underwater 3D scene reconstruction combining Volume Rendering, Spherical Harmonics, and Bundle Adjustment techniques

[**ğŸ“‘ Paper**]() | [**ğŸ’» Code**](https://github.com/lzx1162131988/VM-SHGS) 

<p align="center">
<img src="./img/process.png" alt="VM-SHGS Overview" style="width: 100%; height: auto;">
</p>

## â­ Method Overview

We propose **VM-SHGS**, an integrated neural rendering framework for underwater 3D scene reconstruction that combines **Volume Rendering**, **Spherical Harmonics (SH)**, and **Bundle Adjustment** techniques to achieve high-quality, real-time 3D scene representation and novel view synthesis.

### ğŸ—ï¸ System Architecture

Our framework consists of two core modules working in synergy:

#### ğŸ”§ **BA-DPCT Module** (Bundle Adjustment - Dense Prediction with Channel and spatial aTtention)
- **ğŸ¯ Differentiable Bundle Adjustment**: End-to-end optimization of camera poses and 3D point clouds
- **ğŸ§  Enhanced Attention DPT**: Dense prediction with integrated channel and spatial attention mechanisms
- **ğŸ“ Multi-view Geometric Consistency**: Ensures geometric accuracy of reconstruction
- **âš¡ Real-time Optimization**: Dynamic pose refinement during training and inference

#### ğŸ¨ **VM-SH Module** (Volume Rendering with Spherical Harmonics)
- **ğŸ¯ Volume Rendering**: Efficient neural volume rendering with 3D Gaussian splatting
- **âœ¨ Spherical Harmonics Lighting**: Advanced lighting modeling using degree-3 spherical harmonics functions  
- **âš¡ CUDA Acceleration**: Custom CUDA operators for high-performance forward/backward computation
- **ğŸ”¬ Multi-Component Loss**: Includes reconstruction, depth supervision, pose consistency, and total variation regularization

### ğŸ”„ Collaborative Workflow
1. **BA-DPCT** performs initial pose estimation and feature tracking
2. **Bundle Adjustment** optimizes camera parameters and 3D point clouds
3. **VM-SH** uses refined poses for neural rendering
4. **Multi-component loss** optimizes the entire system end-to-end

## ğŸ“Š Core Features

### ğŸš€ Technical Highlights
- **ğŸ”„ Modular Architecture**: Dual-module integrated framework (BA-DPCT + VM-SH)
- **ğŸ¯ High-Precision Poses**: Bundle Adjustment achieves 5-15% pose accuracy improvement
- **âš¡ High-Performance Rendering**: CUDA-accelerated backend with PyTorch 2.1.2 support
- **ğŸ” Intelligent Optimization**: Multi-component loss function with optimized weights (0.8:0.1:0.05:0.05)
- **ğŸ”§ Flexible Configuration**: Supports various scene types and datasets
- **ğŸ“º Real-time Interaction**: Interactive visualization >30 FPS at 1080p
- **ğŸ† High-Quality Reconstruction**: Photorealistic novel view synthesis with PSNR >28.5 dB

### ğŸ”„ BA-DPCT Core Functions
- **ğŸ“ Precise Tracking**: Transformer-based feature point tracking
- **ğŸ—ºï¸ Pose Estimation**: Multi-view camera pose estimation
- **ğŸ”„ Differentiable BA**: End-to-end trainable Bundle Adjustment
- **ğŸ§  Enhanced Attention**: Channel + spatial attention mechanisms
- **ğŸ“Š Depth Prediction**: High-precision dense depth prediction

### ğŸ¨ VM-SH Core Functions
- **ğŸ”· 3D Gaussian Splatting**: Efficient volumetric representation and rendering
- **âœ¨ Spherical Harmonics Lighting**: Degree-3 spherical harmonics lighting modeling
- **ğŸš€ CUDA Acceleration**: Custom high-performance computational operators
- **ğŸ”¬ Intelligent Loss**: Four-component composite loss function
- **ğŸ¥ Real-time Rendering**: High frame rate novel view synthesis

### ğŸ”¬ Enhanced Loss Function
Our optimized multi-component loss function achieves superior reconstruction quality:

```
L_total = 0.8Ã—L_recon + 0.1Ã—L_depth + 0.05Ã—L_pose + 0.05Ã—L_tv
```

**Component Details:**
- **Reconstruction Loss (80%)**: L2 + SSIM for high-fidelity image reconstruction
- **Depth Supervision Loss (10%)**: L1 geometric constraint guidance with valid depth masking
- **Pose Consistency Loss (5%)**: Multi-view geometric consistency constraints
- **Total Variation Loss (5%)**: 3D smoothness regularization: `L_TV = (1/|V|) Ã— Î£ âˆš(Î”xÂ² + Î”yÂ² + Î”zÂ²)`

## ğŸ”§ Installation

Our framework is based on [WaterSplatting](https://github.com/water-splatting/water-splatting) and [VGGT](https://github.com/facebookresearch/vggt.)

### Create Environment
```bash
# Create conda environment
conda create --name vm_shgs -y python=3.8
conda activate vm_shgs
python -m pip install --upgrade pip
```

### Install Dependencies
```bash
# Install PyTorch with CUDA 11.8
pip install torch==2.1.2+cu118 torchvision==0.16.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118

# Install CUDA Toolkit
conda install -c "nvidia/label/cuda-11.8.0" cuda-toolkit

# Install build tools
pip install ninja cmake

# Install project dependencies
cd VM-SH && pip install -r requirements.txt
cd ../BA-DPCT && pip install -r requirements.txt

# Install VM-SH framework
cd ../VM-SH && pip install --no-use-pep517 -e .

# Install BA-DPCT framework  
cd ../BA-DPCT && pip install -e .

# Install Tiny CUDA NN (optional, for acceleration)
pip install ninja git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch
```

**System Requirements:**
- Python 3.8+
- CUDA 11.8 compatible GPU
- PyTorch 2.1.2
- 16GB+ GPU memory recommended

## ğŸš€ Quick Start

###  Basic Usage Workflow

**Step 1: Prepare COLMAP Data**
```bash
# Use COLMAP to generate dataset
colmap automatic_reconstructor \
  --image_path <path_to_images> \
  --workspace_path <workspace> \
  --dense_path <dense_output>
```

**Step 2: BA-DPCT Pose Estimation**
```bash
cd BA-DPCT
# Basic camera pose estimation
python demo_colmap.py \
  --scene_dir <path_to_colmap_dataset> \
  --output_dir <ba_output>

# Use Bundle Adjustment optimization
python demo_bundle_adjustment.py \
  --mode inference \
  --scene_dir <path_to_colmap_dataset> \
  --use_ba \
  --output_dir <refined_output>
```

**Step 3: VM-SH Neural Rendering**
```bash
cd ../VM-SH
# Train using refined poses
python train.py \
  -s <refined_output> \
  --recon_loss_weight 0.8 \
  --depth_loss_weight 0.1 \
  --pose_consistency_weight 0.05 \
  --tv_loss_weight 0.05

# Evaluate model
python eval.py -m <trained_model_path>

# Generate novel views
python render.py -m <trained_model_path> --output_path renders/
```

## ğŸ“‹ Dataset Format

### Integrated Dataset Structure
Our integrated framework supports the following dataset structure:

```
<dataset_path>
|---images/                    # Original images
|   |---IMG_001.jpg
|   |---IMG_002.jpg
|   |---...
|---depths/                    # Depth maps (optional)
|   |---IMG_001_depth.png
|   |---IMG_002_depth.png
|   |---...
|---sparse/                    # COLMAP sparse reconstruction
|   |---0/
|       |---cameras.bin
|       |---images.bin
|       |---points3D.bin
|---ba_dpct_output/           # BA-DPCT output
|   |---refined_poses.json
|   |---track_results.json
|   |---depth_predictions.npy
|---vm_sh_output/             # VM-SH output
    |---point_cloud.ply
    |---model_params.pth
    |---renders/
```

### BA-DPCT Data Format

**Input Format:**
- **Images**: JPG, PNG (RGB, adaptive resolution)
- **Camera Parameters**: COLMAP binary format
- **Tracking Points**: 2D pixel coordinates (x, y)

**Output Format:**
```json
{
  "refined_poses": {
    "image_id": {
      "rotation": [4x4 matrix],
      "translation": [3x1 vector],
      "confidence": float
    }
  },
  "bundle_adjustment_stats": {
    "iterations": int,
    "final_loss": float,
    "pose_improvement": float
  }
}
```

### VM-SH Data Format

**Input Format:**
- **Images**: JPG, PNG (RGB, RGBA)
- **Depths**: PNG, EXR (single channel, meters)
- **Cameras**: COLMAP binary format or BA-DPCT refined results
- **Trajectories**: JSON format (for video rendering)

**Output Format:**
- **3D Model**: PLY point cloud format
- **Render Parameters**: PyTorch model file
- **Novel View Renders**: PNG/JPG image sequences

### Supported Datasets
- **Custom Datasets**: Supports any COLMAP format dataset
- **Public Datasets**: NeRF Synthetic, LLFF, Tanks & Temples
- **Underwater Scenes**: Specially optimized underwater 3D reconstruction datasets

## ğŸ“ˆ Performance Comparison

For specific indicators, please refer to the paper

## ğŸ› ï¸ Module Features

### ğŸ”§ BA-DPCT Module

**Core Features:**
- âœ… Differentiable Bundle Adjustment optimization
- âœ… Enhanced attention DPT architecture
- âœ… Real-time pose refinement
- âœ… Multi-view geometric consistency
- âœ… End-to-end trainable

**Configuration Parameters:**
```python
from ba_dpct.models.ba_dpct import BA_DPCT

# Create model
model = BA_DPCT(
    img_size=518,
    enable_bundle_adjustment=True,     # Enable BA
    ba_iterations=5,                   # BA iteration count
    ba_learning_rate=1e-3,            # BA learning rate
    use_enhanced_dpt_attention=True,   # Enhanced attention
    dpt_channel_reduction_ratio=16,    # Channel attention reduction ratio
    dpt_spatial_kernel_size=7,         # Spatial attention conv kernel
)
```

### ğŸ¨ VM-SH Module

**Core Features:**
- âœ… 3D Gaussian splatting rendering
- âœ… Degree-3 spherical harmonics lighting modeling
- âœ… CUDA custom operator acceleration
- âœ… Four-component composite loss function
- âœ… Real-time interactive rendering

**Configuration Parameters:**
```python
from vm_sh.vm_sh_model import VMSHModelConfig

# Create configuration
config = VMSHModelConfig(
    num_steps=15000,
    # Composite loss function weights
    recon_loss_weight=0.8,           # Reconstruction loss
    depth_loss_weight=0.1,           # Depth supervision  
    pose_consistency_weight=0.05,    # Pose consistency
    tv_loss_weight=0.05,             # Total variation
    # Medium tensor settings
    medium_grid_size=[128, 128, 128],
    medium_density_n_comp=8,
    medium_app_n_comp=24,
)
```
## ğŸ“ Contact

Feel free to contact us for any questions or collaborations!

## ğŸ¤ Acknowledgments

Our work builds upon excellent open-source projects:

- [**WaterSplatting**](https://github.com/water-splatting/water-splatting) - Core rendering techniques
- [**TensoRF**](https://github.com/apchenstu/TensoRF) - Core rendering techniques
- [**vggt**](https://github.com/facebookresearch/vggt) - 3dgs initialization technology
- [**Nerfstudio**](https://github.com/nerfstudio-project/nerfstudio) - Training and evaluation framework  
- [**Tiny CUDA NN**](https://github.com/NVlabs/tiny-cuda-nn) - High-performance neural networks
- [**PyTorch**](https://pytorch.org/) - Deep learning framework
- [**COLMAP**](https://colmap.github.io/) - Structure-from-Motion
- [**OpenCV**](https://opencv.org/) - Computer vision utilities

Special thanks to the open-source community for advancing neural rendering research!

## ğŸ“„ License



## ğŸ“š Citation


